{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800001d6-23b6-4c4a-85b2-a2567cde9b74",
   "metadata": {},
   "source": [
    "This notebook guides you through the steps to integrate the vector database (Oracle Database 23ai in our case) and retrieve a list of text chunks that are close to the \"question\" in vector space. Then, we will use the most relevant text chunks to create an LLM prompt and ask the Oracle Generative AI Service to create a nicely worded response for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80db670e-f15c-41dc-af16-41c40b9fdebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'faqs'\n",
    "topK = 3\n",
    "\n",
    "sql = f\"\"\"select payload, vector_distance(vector, :vector, COSINE) as score\n",
    "from {table_name}\n",
    "order by score\n",
    "fetch approx first {topK} rows only\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab445e4-c61d-47b3-ad42-91d928cb9343",
   "metadata": {},
   "source": [
    "In the above SQL query, topK represents the number of top results to retrieve. The query selects the payload column along with the cosine distance between the vector column in the specified table {table_name} and a provided vector parameter :vector, aliasing the distance calculation as score.\n",
    "By ordering the results by the calculated score and using fetch approx first {topK} rows only, the query efficiently retrieves only the top topK results based on their cosine similarity to the provided vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1284757b-8717-4e28-bed8-1ee3ae5b09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "question = 'What is Always Free?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ddd68e-fe5b-4dc7-acba-4cc0211a9175",
   "metadata": {},
   "source": [
    "We will then define the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88b3cd0-85e9-4337-a1d8-d6582062134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Oracle Database 23ai\n",
    "un = \"vector\"\n",
    "pw = \"vector\"\n",
    "cs = \"localhost/FREEPDB1\"\n",
    "\n",
    "import oracledb\n",
    "\n",
    "connection = oracledb.connect(user=un, password=pw, dsn=cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46eeaea-752c-4314-8d5a-0f2cdc61bee2",
   "metadata": {},
   "source": [
    "oracledb.connect()function establishes a connection to an Oracle database using the provided credentials and connection details. The function takes username, password and dsn. The DSN (data source name) specifies the host, port and database service name to connect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec0d494-a9c4-453c-8795-e6ef8a8ef4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opc/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "encoder = SentenceTransformer('all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3eccf2-8794-4239-84ba-cfaa48a2de55",
   "metadata": {},
   "source": [
    "We need an encoder to handle the vectorization for us. all-MiniLM-L12-v2 is a specific pre-trained model that is designed to be an encoder. It is based on the MiniLM (Mini Language Model) architecture, which is a lightweight version of transformer models like BERT.\n",
    "Note:\n",
    "â€¢\tIgnore the warning saying IProgress not found., among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f912176-1d36-4d7f-b4b6-10206418ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Code\n",
    "import array\n",
    "import json\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "  embedding = list(encoder.encode(question))\n",
    "  vector = array.array(\"f\", embedding)\n",
    "\n",
    "  results  = []\n",
    "\n",
    "  for (info, score, ) in cursor.execute(sql, vector=vector):\n",
    "      text_content = info.read()\n",
    "      results.append((score, json.loads(text_content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf95caf-eedb-441a-a313-e1c52c204d88",
   "metadata": {},
   "source": [
    "Next, we write the retrieval code. We employ the same encoder as in previous text chunks, generating a vector representation of the question.\n",
    "The SQL query is executed with the provided vector parameter, fetching relevant information from the database. For each result, the code retrieves the text content, stored in JSON format, and appends it to a list along with the calculated similarity score. This process iterates through all fetched results, accumulating them in the results list. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c47623-f9e7-4e2b-ba86-e65300d4103d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.342059164223519,\n",
      "  {'text': 'faq | What are Always Free services?\\n'\n",
      "           '\\n'\n",
      "           'Always Free services are part of Oracle Cloud Free Tier. Always '\n",
      "           'Free services are available for an unlimited time. Some '\n",
      "           'limitations apply. As new Always Free services become available, '\n",
      "           'you will automatically be able to use those as well.\\n'\n",
      "           '\\n'\n",
      "           'The following services are available as Always Free:\\n'\n",
      "           '\\n'\n",
      "           'AMD-based Compute\\n'\n",
      "           'Arm-based Ampere A1 Compute\\n'\n",
      "           'Block Volume\\n'\n",
      "           'Object Storage\\n'\n",
      "           'Archive Storage\\n'\n",
      "           'Flexible Load Balancer\\n'\n",
      "           'Flexible Network Load Balancer\\n'\n",
      "           'VPN Connect\\n'\n",
      "           'Autonomous Data Warehouse\\n'\n",
      "           'Autonomous Transaction Processing\\n'\n",
      "           'Autonomous JSON Database\\n'\n",
      "           'NoSQL Database (Phoenix Region only)\\n'\n",
      "           'APEX Application Development\\n'\n",
      "           'Resource Manager (Terraform)\\n'\n",
      "           'Monitoring\\n'\n",
      "           'Notifications\\n'\n",
      "           'Logging\\n'\n",
      "           'Application Performance Monitoring\\n'\n",
      "           'Service Connector Hub\\n'\n",
      "           'Vault\\n'\n",
      "           'Bastions\\n'\n",
      "           'Security Advisor\\n'\n",
      "           'Virtual Cloud Networks\\n'\n",
      "           'Site-to-Site VPN\\n'\n",
      "           'Content Management Starter Edition\\n'\n",
      "           'Email Delivery',\n",
      "   'path': 'faq'}),\n",
      " (0.483266919331674,\n",
      "  {'text': 'faq | Are Always Free services available for paid accounts?\\n'\n",
      "           '\\n'\n",
      "           'Yes, for paid accounts using universal credit pricing.',\n",
      "   'path': 'faq'}),\n",
      " (0.4878003507147698,\n",
      "  {'text': 'faq | Could you elaborate on the concept of Always Free resources '\n",
      "           'in Oracle Cloud Infrastructure, and how can users leverage them '\n",
      "           'for various use cases while staying within the specified '\n",
      "           'limitations?\\n'\n",
      "           '\\n'\n",
      "           'Always Free resources in Oracle Cloud Infrastructure are services '\n",
      "           'and resources that can be used without incurring charges, subject '\n",
      "           'to certain usage limitations. Users can leverage these resources '\n",
      "           'for development, testing, small-scale applications, and learning '\n",
      "           'purposes, all while adhering to the restrictions outlined in the '\n",
      "           'terms and conditions.',\n",
      "   'path': 'faq'})]\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "import pprint\n",
    "pprint.pp(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db50be-c7c1-429c-bd2e-89eef02b0c2f",
   "metadata": {},
   "source": [
    "Next we print the results. We should have the \"score\" of each hit, which is essentially the distance in vector space between the question and the text chunk, as well as the metadata JSON embedded in each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44f8432d-5dfb-4449-865c-409d22f73b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizerFast\n",
    "import sys\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "\n",
    "\n",
    "tokenizer.model_max_length = sys.maxsize\n",
    "\n",
    "def truncate_string(string, max_tokens):\n",
    "    # Tokenize the text and count the tokens\n",
    "    tokens = tokenizer.encode(string, add_special_tokens=True) \n",
    "    # Truncate the tokens to a maximum length\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    # transform the tokens back to text\n",
    "    truncated_text = tokenizer.decode(truncated_tokens)\n",
    "    return truncated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11709816-fec2-4d29-b488-8c1aec1a4635",
   "metadata": {},
   "source": [
    "Before sending anything to the LLM, we must ensure that our prompt does not exceed the maximum context length of the model. We are planning to use LLaMA 2, so the context is limited to 4,096 tokens. Note that the context is used for both the input tokens (the prompt) and the response. \n",
    "Above code leverages the Hugging Face Transformers library to tokenize text using the LlamaTokenizerFast model. The tokenizer is initialized from the pre-trained hf-internal-testing/llama-tokenizer model, and its model_max_length attribute is set to sys.maxsize to handle extremely large inputs without length constraints.\n",
    "The truncate_string function takes a string and a maximum token count as inputs. It tokenizes the input string, truncates the tokenized sequence to the specified maximum length, and then decodes the truncated tokens back into a string. This function effectively shortens the text to a specified token limit while preserving its readable format, useful for tasks requiring length constraints on input text.\n",
    "Note: Ignore the legacy warning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ce6c92-7ffd-4c03-bf1a-b7d693119f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def loadFAQs(directory_path):\n",
    "   faqs = {}\n",
    "\n",
    "   for filename in os.listdir(directory_path):\n",
    "      if filename.endswith(\".txt\"):  # assuming FAQs are in .txt files\n",
    "         file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "         with open(file_path) as f:\n",
    "            raw_faq = f.read()\n",
    "\n",
    "         filename_without_ext = os.path.splitext(filename)[0]  # remove .txt extension\n",
    "         faqs[filename_without_ext] = [text.strip() for text in raw_faq.split('=====')]\n",
    "\n",
    "   return faqs\n",
    "\n",
    "faqs = loadFAQs('./txt-docs')\n",
    "\n",
    "docs = [{'text': filename + ' | ' + section, 'path': filename} for filename, sections in faqs.items() for section in sections]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f9cf7-6d91-4d49-9208-103fa193e2b0",
   "metadata": {},
   "source": [
    "We will now read, split and store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248b1384-ed5e-4def-84c9-2be9814d05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform docs into a string array using the \"paylod\" key\n",
    "docs_as_one_string = \"\\n=========\\n\".join([doc[\"text\"] for doc in docs])\n",
    "docs_truncated = truncate_string(docs_as_one_string, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb6051-305b-4073-b3bf-231d533f55fc",
   "metadata": {},
   "source": [
    "We will truncate our chunks to 1000 tokens, to leave plenty of space for the rest of the prompt and the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1970a573-8ccd-4999-8716-f69948ccba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM Prompt\n",
    "prompt = f\"\"\"\\\n",
    "    <s>[INST] <<SYS>>\n",
    "    You are a helpful assistant named Oracle chatbot. \n",
    "    USE ONLY the sources below and ABSOLUTELY IGNORE any previous knowledge.\n",
    "    Use Markdown if appropriate.\n",
    "    Assume the customer is highly technical.\n",
    "    <</SYS>> [/INST]\n",
    "\n",
    "    [INST]\n",
    "    Respond to PRECISELY to this question: \"{question}.\",  USING ONLY the following information and IGNORING ANY PREVIOUS KNOWLEDGE.\n",
    "    Include code snippets and commands where necessary.\n",
    "    NEVER mention the sources, always respond as if you have that knowledge yourself. Do NOT provide warnings or disclaimers.\n",
    "    =====\n",
    "    Sources: {docs_truncated}\n",
    "    =====\n",
    "    Answer (Three paragraphs, maximum 50 words each, 90% spartan):\n",
    "    [/INST]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e79e43-12f0-4388-8e88-f84d86155682",
   "metadata": {},
   "source": [
    "The prompt will include the retrieved top chunks, the question posed by the user, and the custom instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f32a6d-67c3-41b0-9211-25ba26fdd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "from LoadProperties import LoadProperties\n",
    "\n",
    "# Setup basic variables\n",
    "properties = LoadProperties()\n",
    "\n",
    "# Use Instance Principals for Authentication\n",
    "signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()\n",
    "\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config={}, signer=signer, service_endpoint=properties.getEndpoint(), retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "chat_request.message = prompt\n",
    "chat_request.max_tokens = 1000\n",
    "chat_request.temperature = 0.0\n",
    "chat_request.frequency_penalty = 0\n",
    "chat_request.top_p = 0.75\n",
    "chat_request.top_k = 0\n",
    "\n",
    "chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=properties.getModelName())\n",
    "chat_detail.chat_request = chat_request\n",
    "chat_detail.compartment_id = properties.getCompartment()\n",
    "chat_response = generative_ai_inference_client.chat(chat_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094c0fa-5f3c-46f7-b18a-81afedab42e3",
   "metadata": {},
   "source": [
    "We will now call the OCI Generative AI Chat model and store the chat modelâ€™s response in chat_response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d11e612-d844-40ee-8cd1-2bdf280e4f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Always Free is a program offered by Oracle Cloud. It provides users with '\n",
      " 'access to a range of services that are completely free to use indefinitely. '\n",
      " 'These include compute, storage, and networking resources. \\n'\n",
      " '\\n'\n",
      " 'Always Free is ideal for developers who want to experiment, build, and test '\n",
      " 'applications in the cloud without incurring any costs. The program is '\n",
      " 'available to anyone, and you can sign up for it on the Oracle Cloud Free '\n",
      " 'Tier webpage.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(\n",
    "    chat_response.data.chat_response.chat_history[1].message\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3541dd2-e3a7-4cc3-9730-5ed6222d4ef2",
   "metadata": {},
   "source": [
    "The response is extracted and cleaned of any leading or trailing whitespace before being printed in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39648b4f-292c-4ea9-9a77-e4e32d1105d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
