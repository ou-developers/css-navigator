{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3737666-7805-4171-8b4e-558823de04f4",
   "metadata": {},
   "source": [
    "This notebook guides you through the code required to load and parse a text FAQ from a file named faq.txt from a local folder, split them into segments, and then create embeddings and ingest them into the vector database. You can use these embeddings for finding similarity in phrases that are similar in context or category. Embeddings are typically stored in a vector database which in this case Oracle Database 23ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef9b625-4aaf-49e5-a702-a6c6dfbfe40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the source file and store chunks in an array\n",
    "import os\n",
    "\n",
    "def loadFAQs(directory_path):\n",
    "   faqs = {}\n",
    "\n",
    "   for filename in os.listdir(directory_path):\n",
    "      if filename.endswith(\".txt\"):  # assuming FAQs are in .txt files\n",
    "         file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "         with open(file_path) as f:\n",
    "            raw_faq = f.read()\n",
    "\n",
    "         filename_without_ext = os.path.splitext(filename)[0]  # remove .txt extension\n",
    "         faqs[filename_without_ext] = [text.strip() for text in raw_faq.split('=====')]\n",
    "\n",
    "   return faqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cb969-2c7d-4b9f-b344-a3387a3a9a22",
   "metadata": {},
   "source": [
    "The function will open all the .txt files in a specified folder which is txt-docs, read them, split the content using the ======== separator. It will then put all the resulting chunks in an array.\n",
    "The array is stored inside a dictionary with the file name used as the key. This will be useful later if many other FAQ files are available inside the folder, helping to differentiate between the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ccc6ab-cd43-49cc-aa59-5295ba6bd7b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store data from txt-docs\n",
    "faqs = loadFAQs('./txt-docs')\n",
    "faqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28e944-3d2a-4a06-862d-3a13a93fc596",
   "metadata": {},
   "source": [
    "We will call the loadFAQs function above with our data stored in txt-docs folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57a5e2-e0d0-40f8-8b9d-d6cd32dd7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [{'text': filename + ' | ' + section, 'path': filename} for filename, sections in faqs.items() for section in sections]\n",
    "\n",
    "# Sample the resulting data\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02da7c-6b01-41cd-a22c-f8b332a8e712",
   "metadata": {},
   "source": [
    "We will now prepare the source data to arrange the above dictionary in a way that is easy to ingest in the vector database.  This will store the data in the following format:\n",
    "[{'text': 'faq | Who are you and what can you do?\\n\\nI am DORA, the Digital ORacle Assistant, a digital assistant working for Oracle EMEA. I can answer questions about Oracle Cloud (OCI) and especially about the Free Trial and Always Free programs.',\n",
    "  'path': 'faq'}]\n",
    "As you can see, for each chunk, we prepend the name of the source file to the text content, in our case, \"faq |\". This is a very simple way to ensure that some context is preserved with each chunk and vectorized later. It will help tremendously in the retrieval stage, when vector distances will be calculated between the question and each chunk.\n",
    "Also, we state the name of the source file in the path component. It will be useful later if we want to display a link back to the source when using a chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1048ba-e8d0-44a6-828e-df304cbc59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Oracle Database 23ai\n",
    "un = \"vector\"\n",
    "pw = \"vector\"\n",
    "cs = \"localhost/FREEPDB1\"\n",
    "\n",
    "import oracledb\n",
    "\n",
    "connection = oracledb.connect(user=un, password=pw, dsn=cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8dfeb-9925-44f6-a5eb-524f3d7fdb2f",
   "metadata": {},
   "source": [
    "oracledb.connect()function establishes a connection to an Oracle database using the provided credentials and connection details. The function takes username, password and dsn. The DSN (data source name) specifies the host, port and database service name to connect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d156b5-1461-4778-ad74-4d52fbff4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'faqs'\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    # Create the table\n",
    "    create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id NUMBER PRIMARY KEY,\n",
    "            payload CLOB CHECK (payload IS JSON),\n",
    "            vector VECTOR\n",
    "        )\"\"\"\n",
    "    try:\n",
    "        cursor.execute(create_table_sql)\n",
    "    except oracledb.DatabaseError as e:\n",
    "        raise\n",
    "\n",
    "    connection.autocommit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0dba5-06f4-4a9a-8a2a-3027ee348b12",
   "metadata": {},
   "source": [
    "We need a table inside our database to store our vectors and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdfaa3d-6b5e-4755-8f02-7188e202282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "encoder = SentenceTransformer('all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7362f-99e3-4fc0-b733-80e6a224a417",
   "metadata": {},
   "source": [
    "We need an encoder to handle the vectorization for us. all-MiniLM-L12-v2 is a specific pre-trained model that is designed to be an encoder. It is based on the MiniLM (Mini Language Model) architecture, which is a lightweight version of transformer models like BERT.\n",
    "Note:\n",
    "•\tIgnore the warning saying IProgress not found., among others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a190f-8c00-483f-b5d9-2d6df762b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "\n",
    "# Define a list to store the data\n",
    "data = [\n",
    "   {\"id\": idx, \"vector_source\": row['text'], \"payload\": row} \n",
    "   for idx, row in enumerate(docs)\n",
    "]\n",
    "\n",
    "# Collect all texts for batch encoding\n",
    "texts = [f\"{row['vector_source']}\" for row in data]\n",
    "\n",
    "# Encode all texts in a batch\n",
    "embeddings = encoder.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Assign the embeddings back to your data structure\n",
    "for row, embedding in zip(data, embeddings):\n",
    "   row['vector'] = array.array(\"f\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9df9d-04af-4a62-97ac-1eb08594bc67",
   "metadata": {},
   "source": [
    "We go through all our chunks (stored in the docs dictionary) and encode the text content. Now we have a structure with all our chunks, including its context—the source file name, in this simple example—and the vector representation for each of them.\n",
    "encoder.encode() method is used to generate embeddings (vector representations) for the input texts. It uses the encoder object, which is an instance of the SentenceTransformer class, loaded with a specific model (e.g., all-MiniLM-L12-v2).\n",
    "batch_size=32 means that 32 sentences will be processed in parallel during each batch, which can be more efficient than processing each sentence individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda9806-e360-4589-b422-c08de07be676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the chunks + vectors in the database\n",
    "import json\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    # Truncate the table\n",
    "    cursor.execute(f\"truncate table {table_name}\")\n",
    "\n",
    "    prepared_data = [(row['id'], json.dumps(row['payload']), row['vector']) for row in data]\n",
    "\n",
    "    # Insert the data\n",
    "    cursor.executemany(\n",
    "        f\"\"\"INSERT INTO {table_name} (id, payload, vector)\n",
    "        VALUES (:1, :2, :3)\"\"\",\n",
    "        prepared_data\n",
    "    )\n",
    "\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b1a9c7-86f6-4160-8fdd-337223d02464",
   "metadata": {},
   "source": [
    "Initially, we use a cursor object from the established database connection to execute a command that truncates the specified table. This operation ensures that all existing rows are removed, effectively resetting the table to an empty state and preparing it for fresh data insertion.\n",
    "Subsequently, the code prepares a list of tuples containing the new data. Each tuple includes an id, a JSON-encoded payload, and a vector.\n",
    "The json.dumps function is used to convert the payload into a JSON string format, ensuring that complex data structures are properly serialized for database storage.\n",
    "We then utilize the cursor.executemany method to insert all prepared tuples into the table in a single batch operation. This method is highly efficient for handling bulk inserts, significantly reducing the number of database transactions and improving performance. Finally, the connection.commit method is called to commit the transaction, ensuring that all changes are saved and made permanent in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd77ecc-78b6-4afd-98d8-d2f924092d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    # Define the query to select all rows from a table\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Print the rows\n",
    "    for row in rows[:5]:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848977b1-70b3-4fe3-8431-ef2241ebb321",
   "metadata": {},
   "source": [
    "Above will examine what is stored in the table in our database and print first 5 rows. You should be able to see embeddings for the respective text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d459d9-f1e6-4ddb-a950-b33587b555d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
